
Just seven questions I will turn it into a blog post.

***
q1. What project have you worked on do you wish you could go back to,
and do better?

I started my career as a quant in a small startup hedge fund. I
developed time series models to forecast short-term volatility in
equities and equity indices as part of an option trading strategy. It
is a fascinating topic and I have been working on it for years,
finally discovering R and then switching over almost all the logic to
it. Thinking back, I would re-engineer large portions of it and I made
a ton of mistakes on both the modelling and implementation side. For
example, the system automatically generates PDF reports of the
forecasts but it does so by hand creating latex files that are then
compiled into PDF. One of the first things I would do is switch all
that over to use either 'knitr' or 'rmarkdown'.

That said, I had worked on the modelling for a long time, so I am
fairly happy with the basic model, though there was still loads of
extra stuff to both investigate and implement.

On the modelling side, I worked on a persistency model using survival
analysis, which is how I learned about the subject in the first
place. As a result, there are a lot of different things I would love
to return to and do differently. In retrospect, I was too quick to
move past the simpler models. We could see the assumptions were not
consistent with the data, and so did not fully explore simpler
approaches. I am now curious to learn what insights those simpler
approaches would yield.

It is such a universal problem (customer churn) that I expect I will
be involved in something similar in the future and apply those lessons
then.



***
q2. What advice do you have to younger analytics professionals and in
particular PhD students in the Sciences?

I think the key advice I would give is the same for everyone - never
stop learning. This may be the availability heuristic at play with me,
but I have never seen a connection between qualifications and the
quality of the analyst. All the good analysts I have met are so
because they have initiative, bot because of an academic qualification.

Initiative manifests in many ways. First, when they encounter a
problem they learn what they need to do and get on with it. Second,
much of their knowledge is self-taught. Finally, and I believe most
importantly, they have an inherent curiosity - the best analysts I
know engage in the field in their own time, mainly because they want
to.

This brings up a related issue I have been pondering for some time. I
am ambitious. I want to be a top data scientist some day. I have no
academic ambition whatsoever, but my goal is to be able to hold my own
in any conversation with anyone in the field.

How do I achieve this? What do I need to do to get to that point?

While probably not as keen as the average fan, I love sport - soccer,
the NFL and Gaelic Football in particular. For anyone who has met me
in person, comparing me to a top athelete seems preposterous, but
there is a lot to be learned from top atheletes if you want to excel
at your chosen field. Look at how they prepare and train. These
principles almost certainly apply to other professions too, but it is
more fun to talk about sport. :)

When I read about Lionel Messi, Tom Brady or Colm Cooper (for our
non-Irish readers the recently-retired 'Gooch' is arguably the
greatest GAA player to ever play the game - he was majestic to watch),
the one thing that always stands out for me is their fanatical
devotion to their chosen career not their obvious talent. All their
team-mates mention how hard they worked despite their abundance of
natural advantages. Players with huge natural talent often coast, but
elite players are the opposite - they work as hard as the fringe
players slogging to merely survive the cut.

In our field, the analogy is we need to be constantly working on
improving - going to Meetups, reading about new techniques, watching
videos on YouTube and looking to strengthen areas where you are
weak. This is why a natural interest and curiousity is so invaluable -
it makes these necessary tasks much less of a burden as they are
things you would want to do anyway.

Secondly, top players do the simple things well, almost never making a
mistake. They are fallible of course, and make mistakes, but almost
never on the basics. They are rigourous about practicing the basic
skills and principles, and that is why they are so good. The bread and
butter of their craft is second-nature to them.

This is why I focus so much on basic statistics classes and reread and
rewatch the books and lectures I find useful. I want these things to
be second nature and they are not.

Probability and statistics are so counter-intuitive that I almost
never get things right on gut feeling. I am almost always wrong. So
much so that I gave a talk about probabilistic graphical models about
a year ago and during the questions at the end made an off-hand joke
about going with the opposite of my intution. It was said in jest at
the time but is sadly true!



***
q3. What do you wish you knew earlier about being a data scientist?

I have two main things I wish I learned early on in my career, and
both are connected philosophically. First, I wish I had learned about
probabilistic thinking, risk management, economics and statistics -
you can never learn enough about these fundamental topics. Secondly, I
wish I learned it is okay to start working with a bad model that you
know is wrong but simple.

To that first point, I deliberately spend a long time fighting my
natural desire for a clean, elegant and correct answer to a problem. I
would work on a problem, get to a point that I was confident pointed
us in the right direction, but then realise that 'proving' this was
right involved a huge amount of time and effort, assuming it was
possible.

I attributed my natural reluctance to pursue this 'answer' as
laziness, and felt guilty. I felt I was being unprofessional and
sloppy. But working on forecasting models for trading taught me that
this was not the case. Models are so imperfect, with so many
compromises that it is generally more optimal to think about other
things first - what are the limitations of the model in practice, what
is it saying, how are you going to use it. Answer those questions
first, THEN worry about improving it.

This is why I always start with simple, stupid, wrong models. They are
quick to produce, they help you learn a lot about what you are doing,
they fail in spectacular ways and they are sometimes all you need. In
terms of costs and benefits, they are hard to beat.

Then again, maybe I am just a lazy sod looking to excuse his
sloppiness. :)




***
q4. How do you respond when you hear the phrase 'big data'?

I hate it because it has become a meaningess buzzword largely used as
a means of making sales. It also seems to be much more in the realm of
engineering than statistics or data analysis.

My attitude to the term is best summarised by the interview you had
with Hadley Wickham a while back: there are three categories of data
size, in-memory, on-disk and then truely massive problems like
recommender systems etc. I think a very large majority of problems can
be solved by appropriate sampling of your data down to a manageable
size and then analysing those subsets.

The whole point of statistics is to make inferences about a population
from a sample of the data.

Of course, once decided on a solution, putting the model into
production and scaling it for your business is also an issue, but that
is a problem more belonging to the realm of network and software
engineering. That said, it is important to keep people with a solid
understanding of the concepts stay involved, just in case some
'optimisations' ruin the output.



***
q5. What is the most exciting thing about your field?

Robert McNamara in 'The Fog of War' mentioned that you should never
answer the question asked but instead answer the question you wanted
to be asked, so with your forebearance I will first answer a liberal
interpretation of that question: what work gets me excited?

The short answer to that question is varied, all sorts of things do,
but it tends to be very specific thing I am working on. In the last
few months, I was excited to try out dataexpks on a brand new data set
to see what it showed me and how well it worked. I love think of ways
to use Monte Carlo simulation to test the output of various regression
models, and over Xmas I was fascinated by working on different ways to
investigate differences between a subpopulation within a larger
population.

I love discovering new ways to learn the basics - there are a few
excellent ones out there and I read them all the time. I can never
learn enough about ways to think about the basics as in my experience
reality tends to present us with basic statistical problems in new and
unusual ways.

Having multiple perspectives and multiple approaches is invaluable in
those situations.

Regarding your original question as I think you intended, I think the
advances in reinforcement learning techniques probably have the
biggest potential - some of the Atari gameplaying stuff from Deep Mind
was eye-opening. Sadly, if history is any guide, much of it will prove
to be hype, but I imagine some very interesting results to come from
the work.



***
q6. How do you go about framing a data problem - in particular, how do
you avoid spending too long, how do you manage expectations etc. How
do you know what is good enough?

Framing a data problem is a tough one to answer - I do not really know
how I do it. I have had the good fortune to help a lot of people with
their projects and problems, exposing me to a wide variety of
problems, and I learned something from all of them. I also read a lot
of blogs, articles and subscribe to mailing lists. I may not have time
to read all this, but often all I need is a word to Google in the
future. That will get you started.

As a result, the first thing I focus on is understanding the problem:
what is being asked? Do we have any data? What does is it look like?
Are there other data available we can use to enrich or use as a
substitute?

Going through that process will suggest approaches to use, and at that
point I draw upon previous experience, however superfical.

By keeping this focus, your other questions are straightforward to
answer: if the current model is not likely to improve the answer by an
amount relevant to the goal, it is not worth spending more time
on. Similarly, knowing what is needed will tell you if your current
model is good enough, or often if there is a model that is good enough
- it is possible the level of accuracy required is not feasible.

In the latter case, discovering that early is much better than later -
you know not to waste time, money and resources on a lost cause.



***
q7. You've spoken before about the 'need for apprenticeships' in Data
Science. Do you have any suggestions on what that would involve? Are
meetups and coaching a good first start?

To explain the point I was making on that note, I think there is a lot
of implicit knowledge in this field, and I have been told a number of
times from people looking for help that people feel overwhelmed by the
sheer amount of knowledge people feel they need to know.

I do not think this is true, but I understand where this comes from:
there is so many different aspects to working with data it is tough to
know where to start. I always start very simple, but as I mentioned
early, it took a lot of time, thought and effort to get to that point,
and it is not easy to explain these ideas in theory - you have to work
on a number of different datasets to get a feel for how to do this.

As a result, I believe an approach such as mentoring or
apprenticeships are a good way to teach people - more experienced
analysts can guide junior members around the various pitfalls and
traps that are easy to fall into. It also allows us to show people
that you do not need fancy and sophisticated techniques and algorithms
to do interesting work - some of the most interesting work I have seen
involved little more than summary statistics and basic models like
linear regression and decision trees.

All this stuff is very hard to learn from a book - almost
improssible. The closest book I read that talks about this is Data
Analysis Using Regression and Multilevel/Hierarchical Models by Gelman
and Hill, stressing the importance of starting from simple models. I
highly recommend this book.

That said, I could only appreciate the point because I was already
experienced, a younger version of myself would have missed the
point. I would have needed 'permission' from a more experienced
analyst to think it was good to do such a thing.
